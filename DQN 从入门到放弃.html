<html>
<head>
  <title>DQN 从入门到放弃</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/307027 (zh-CN, DDL); Windows/6.1.0 (Win32);"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="752"/>
<h1>DQN 从入门到放弃</h1>

<div>
<span><div><div><h1><span style="color: rgb(28, 51, 135);">DQN与增强学习</span></h1><h2><span style="color: rgb(28, 51, 135);">增强学习是什么</span></h2><div>        在人工智能领域，一般用<span style="font-weight: bold;">智能体Agent</span>来表示一个具备行为能力的物体，比如机器人，无人车，人等等。那么增强学习考虑的问题就是<span style="font-weight: bold;">智能体Agent</span>和<span style="font-weight: bold;">环境Environment</span>之间交互的任务。比如一个机械臂要拿起一个手机，那么机械臂周围的物体包括手机就是环境，机械臂通过外部的比如摄像头来感知环境，然后机械臂需要输出动作来实现拿起手机这个任务。</div><div>        那么，不管是什么样的任务，都包含了一系列的<span style="font-weight: bold;">动作Action</span>,<span style="font-weight: bold;">观察Observation</span>还有<span style="font-weight: bold;">反馈值Reward</span>。所谓的Reward就是Agent执行了动作与环境进行交互后，环境会发生变化，<font style="font-size: 12pt;"><span style="font-size: 12pt; color: rgb(173, 0, 0); font-weight: bold;">变化的好与坏就用Reward来表示</span></font>。</div><div>        接下来这里用了Observation观察一词而不是环境那是因为Agent不一定能得到环境的所有信息，比如机械臂上的摄像头就只能得到某个特定角度的画面。因此，只能用Observation来表示Agent获取的感知信息。</div><div><br/></div><div><br/></div><div><font style="font-size: 14pt;"><span style="font-size: 14pt; color: rgb(50, 135, 18); font-weight: bold;">任务的目标就出来了，那就是要能获取尽可能多的Reward。</span></font></div><div><br/></div><div>        状态与动作的关系其实就是输入与输出的关系，而状态State到动作Action的过程就称之为一个<span style="font-weight: bold;">策略Policy，</span>一般用<img src="http://www.zhihu.com/equation?tex=%5Cpi+"></img>表示，也就是需要找到以下关系：<img src="http://www.zhihu.com/equation?tex=a%3D%5Cpi%28s%29" style="line-height: 1.45;"></img><span style="line-height: 1.45;">或者</span><img src="http://www.zhihu.com/equation?tex=%5Cpi%28a%7Cs%29" style="line-height: 1.45;"></img><span style="line-height: 1.45;">，</span><span style="line-height: 1.45;">其中a是action，s是state。第一种是一一对应的表示，第二种是概率的表示。</span></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><font style="font-size: 18pt;"><span style="font-size: 18pt; color: rgb(28, 51, 135); font-weight: bold;">Bellman方程</span></font></div><div><br/></div><div><img src="http://www.zhihu.com/equation?tex=v%28s%29+%3D+%5Cmathbb+E%5BR_%7Bt%2B1%7D+%2B+%5Clambda+v%28S_%7Bt%2B1%7D%29%7CS_t+%3D+s%5D" width="232"></img></div><div><br/></div><div>        极其简洁，透出的含义就是<font style="font-size: 12pt;"><span style="font-size: 12pt; color: rgb(173, 0, 0); font-weight: bold;">价值函数的计算可以通过迭代的方式来实现</span></font>。接下来本文将介绍如何构建基于Bellman方程的算法及Q-Learning。首先介绍动作价值函数</div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><h2><span style="font-size: 18pt; color: rgb(28, 51, 135);">Action-Value function 动作价值函数</span></h2><div>        每个动作之下的状态又多不一样，我们更关心在某个状态下的不同动作的价值。显然。如果知道了每个动作的价值，那么就可以选择价值最大的一个动作去执行了。</div><div>这就是 Action-Value function<img src="http://www.zhihu.com/equation?tex=Q%5E%5Cpi%28s%2Ca%29"></img>。那么同样的道理，也是使用reward来表示，只是<span style="color: rgb(173, 0, 0); font-weight: bold;">这里的reward和之前的reward不一样</span>，这里是执行完动作action之后得到的reward，之前state对应的reward则是多种动作对应的reward的期望值。显然，动作之后的reward更容易理解。</div><div>        那么，有了上面的定义，动作价值函数就为如下表示：</div><div><br/></div><div style="text-align: center;"><img src="http://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D%0AQ%5E%5Cpi%28s%2Ca%29+%26+%3D++%5Cmathbb+E%5Br_%7Bt%2B1%7D+%2B+%5Clambda+r_%7Bt%2B2%7D+%2B+%5Clambda%5E2r_%7Bt%2B3%7D+%2B+...+%7Cs%2Ca%5D+%5C%5C%5C%5C%0A%26+%3D+%5Cmathbb+E_%7Bs%5E%5Cprime%7D%5Br%2B%5Clambda+Q%5E%5Cpi%28s%5E%5Cprime%2Ca%5E%5Cprime%29%7Cs%2Ca%5D%0A%5Cend%7Balign%7D" style="line-height: 1.45;"></img></div><div style="text-align: left;"><br/></div><div style="text-align: center;"><br/></div><div style="text-align: center;"><br/></div><div style="text-align: left;"></div><h2 style="text-align: left;"><span style="font-size: 18pt; color: rgb(28, 51, 135);">Optimal value function 最优价值函数</span></h2><div style="text-align: left;"><div>        现在求解最优策略等价于求解最优的value function，找到了最优的value function，自然而然策略也就是找到。（当然，这只是求解最优策略的一种方法，也就是value-based approach，由于DQN就是value-based，因此这里只讲这部分，以后我们会看到还有policy-based和model-based方法。一个就是直接计算策略函数，一个是估计模型，也就是计算出状态转移函数，从而整个MDP过程得解</div></div><div style="text-align: left;">        最优动作价值函数和一般的动作价值函数的关系：</div><div style="text-align: left;"><div>            <img src="http://www.zhihu.com/equation?tex=Q%5E%2A%28s%2Ca%29+%3D+%5Cmax_%5Cpi+Q%5E%5Cpi%28s%2Ca%29"></img></div></div><div>        套用上一节得到的value function，可以得到</div><div><br/></div><div>            <img src="http://www.zhihu.com/equation?tex=Q%5E%2A%28s%2Ca%29+%3D+%5Cmathbb+E_%7Bs%5E%5Cprime%7D%5Br%2B%5Clambda+%5Cmax+_%7Ba%5E%5Cprime%7DQ%5E%2A%28s%5E%5Cprime%2Ca%5E%5Cprime%29%7Cs%2Ca%5D%0A"></img></div><div><br/></div><div><br/></div><div><br/></div><h2><span style="font-size: 18pt; color: rgb(28, 51, 135);">策略迭代Policy Iteration</span></h2><div>Policy Iteration的目的是通过迭代计算value function 价值函数的方式来使policy收敛到最优。</div><div style="margin-top: 1em; margin-bottom: 1em;">Policy Iteration本质上就是直接使用Bellman方程而得到的：</div><div><img src="DQN 从入门到放弃_files/Image.jpg" type="image/jpeg" data-filename="Image.jpg" width="1140"/></div><div><br/></div><div>那么Policy Iteration一般分成两步：</div><ol><li>Policy Evaluation 策略评估。目的是 更新Value Function，或者说更好的估计基于当前策略的价值</li><li>Policy Improvement 策略改进。 使用 <font style="font-size: 12pt;"><span style="font-size: 12pt; color: rgb(173, 0, 0); font-weight: bold;">greedy policy 产生新的样本</span></font>用于第一步的策略评估。</li></ol><div><br/></div><div>        <font style="font-size: 12pt;"><span style="font-size: 12pt; color: rgb(173, 0, 0); font-weight: bold;">本质上就是使用当前策略产生新的样本，然后使用新的样本更好的估计策略的价值，然后利用策略的价值更新策略，然后不断反复。理论可以证明最终策略将收敛到最优</span></font></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><h2><span style="font-size: 18pt; color: rgb(28, 51, 135);">Value Iteration 价值迭代</span></h2><div>Value Iteration则是使用Bellman 最优方程得到</div><div><img src="DQN 从入门到放弃_files/Image [1].jpg" type="image/jpeg" data-filename="Image.jpg" width="1180"/>然后改变成迭代形式</div><div><img src="DQN 从入门到放弃_files/Image [2].jpg" type="image/jpeg" data-filename="Image.jpg" width="1166"/></div><div><br/></div><div><br/></div><div><br/></div><div><font color="#328712" style="font-size: 14pt;"><span style="font-size: 14pt; color: rgb(50, 135, 18); font-weight: bold;">理解：</span><span style="font-size: 14pt; color: rgb(50, 135, 18); font-weight: bold;">policy iteration使用bellman方程来更新value，最后收敛的value 即</span><span style="font-size: 14pt; color: rgb(50, 135, 18); font-weight: bold;"><img src="http://www.zhihu.com/equation?tex=v_%5Cpi"></img></span><span style="font-size: 14pt; color: rgb(50, 135, 18); font-weight: bold;">是当前policy下的value值（所以叫做对policy进行评估），目的是为了后面的policy improvement得到新的policy。</span></font></div><div><font color="#328712" style="font-size: 14pt;"><span style="font-size: 14pt; color: rgb(50, 135, 18); font-weight: bold;">        而value iteration是使用bellman 最优方程来更新value，最后收敛得到的value即</span><span style="font-size: 14pt; color: rgb(50, 135, 18); font-weight: bold;"><img src="http://www.zhihu.com/equation?tex=v_%2A"></img></span><span style="font-size: 14pt; color: rgb(50, 135, 18); font-weight: bold;">就是当前state状态下的最优的value值。因此，只要最后收敛，那么最优的policy也就得到的。因此这个方法是基于更新value的，所以叫value iteration。</span></font></div><div><br/></div><div><br/></div><div><br/></div><h2><span style="font-size: 18pt; color: rgb(28, 51, 135);">Q-Learning</span></h2><div>Q Learning的思想完全根据value iteration得到。但要明确一点是value iteration每次都对所有的Q值更新一遍，也就是所有的状态和动作。但事实上在实际情况下我们没办法遍历所有的状态，还有所有的动作，我们只能得到有限的系列样本。因此，只能使用有限的样本进行操作。那么，怎么处理？Q Learning提出了一种更新Q值的办法：</div><div><img src="http://www.zhihu.com/equation?tex=Q%28S_%7Bt%7D%2CA_%7Bt%7D%29+%5Cleftarrow+Q%28S_%7Bt%7D%2CA_%7Bt%7D%29%2B%5Calpha%28%7BR_%7Bt%2B1%7D%2B%5Clambda+%5Cmax+_aQ%28S_%7Bt%2B1%7D%2Ca%29%7D+-+Q%28S_t%2CA_t%29%29"></img></div><div>虽然根据value iteration计算出target Q值，但是这里并没有直接将这个Q值（是估计值）直接赋予新的Q，而是采用渐进的方式类似梯度下降，朝target迈近一小步，取决于α,这就能够减少估计误差造成的影响。类似随机梯度下降，最后可以收敛到最优的Q值。</div><div><br/></div><div>Q-Learning的算法如下：</div><div><img src="DQN 从入门到放弃_files/Image [3].jpg" type="image/jpeg" data-filename="Image.jpg" width="600"/></div><div><ol><li style="text-align: center;">首先就是要确定如何存储Q值，最简单的想法就是用矩阵，一个s一个a对应一个Q值，所以可以把Q值想象为一个很大的表格，横列代表s，纵列代表a，里面的数字代表Q值<img src="DQN 从入门到放弃_files/153a0529f2b86c87d5827999f13a1709_hd.jpg" type="image/jpeg" data-filename="153a0529f2b86c87d5827999f13a1709_hd.jpg"/></li><li>初始化Q矩阵，比如都设置为0<img src="DQN 从入门到放弃_files/Image.png" type="image/png" data-filename="Image.png" style="width: 660px; transform: translate3d(296px, 184.583px, 0px) scale3d(1.83636, 1.83636, 1); opacity: 1;"/></li><li>开始实验。根据当前Q矩阵及<img src="http://www.zhihu.com/equation?tex=%5Cepsilon-greedy"></img>方法获取动作。比如当前处在状态s1，那么在s1一列每一个Q值都是0，那么这个时候随便选择都可以。假设我们选择a2动作，然后得到的reward是1，并且进入到s3状态</li><li>接下来我们要根据<img src="http://www.zhihu.com/equation?tex=Q%28S_%7Bt%7D%2CA_%7Bt%7D%29+%5Cleftarrow+Q%28S_%7Bt%7D%2CA_%7Bt%7D%29%2B%5Calpha%28%7BR_%7Bt%2B1%7D%2B%5Clambda+%5Cmax+_aQ%28S_%7Bt%2B1%7D%2Ca%29%7D+-+Q%28S_t%2CA_t%29%29"></img></li><li style="text-align: left;">接下来就是进入下一次动作，这次的状态是s3，假设选择动作a3，然后得到1的reward，状态变成s1，那么我们同样进行更新：<img src="http://www.zhihu.com/equation?tex=Q%28s_3%2Ca_3%29+%3D+2+%2B+%5Cmax_a+Q%28s_1%2Ca%29+%3D+2+%2B+1+%3D+3" width="290"></img><span style="line-height: 1.45;">所以Q的表格就变成：</span><img src="DQN 从入门到放弃_files/Image [4].jpg" type="image/jpeg" data-filename="Image.jpg" style="line-height: 1.45;" width="416"/></li></ol><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><h2><span style="font-size: 18pt; color: rgb(28, 51, 135);">价值函数近似Value Function Approximation</span></h2><div>        在上面的简单分析中，我们使用表格来表示Q(s,a)，但是这个在现实的很多问题上是几乎不可行的，因为状态实在是太多。使用表格的方式根本存不下。</div><div>        有必要对状态的维度进行压缩，解决办法就是 价值函数近似Value Function Approximation。</div><div>        什么是价值函数近似呢？说起来很简单，就是用一个函数来表示Q(s,a)。即</div><div style="text-align: center;"><img src="http://www.zhihu.com/equation?tex=Q%28s%2Ca%29+%3D+f%28s%2Ca%29"></img></div><div style="margin-top: 1em; margin-bottom: 1em;">f可以是任意类型的函数，比如线性函数：</div><div style="text-align: center;"><img src="http://www.zhihu.com/equation?tex=Q%28s%2Ca%29+%3D+w_1s+%2B+w_2a+%2B+b"></img> </div><div>其中<img src="http://www.zhihu.com/equation?tex=w_1%2Cw_2%2Cb"></img>是函数f的参数。<span style="line-height: 1.45;">如果我们就用</span><img src="http://www.zhihu.com/equation?tex=w" style="line-height: 1.45;"></img><span style="line-height: 1.45;">来统一表示函数f的参数，那么就有</span></div><div style="text-align: center;"><img src="http://www.zhihu.com/equation?tex=Q%28s%2Ca%29+%3D+f%28s%2Ca%2Cw%29"></img></div><div style="margin-top: 1em; margin-bottom: 1em;">为什么叫近似，因为我们并不知道Q值的实际分布情况，本质上就是用一个函数来近似Q值的分布，所以，也可以说是</div><div style="text-align: center;"><img src="http://www.zhihu.com/equation?tex=Q%28s%2Ca%29%5Capprox+f%28s%2Ca%2Cw%29" width="134"></img></div><div><br/></div><div><br/></div><h2><br/></h2><h2><span style="font-size: 18pt; color: rgb(28, 51, 135);">高维状态输入，低维动作输出的表示问题</span></h2><div>        其实就是<img src="http://www.zhihu.com/equation?tex=Q%28s%29+%5Capprox+f%28s%2Cw%29"></img>，只把状态s作为输入，但是输出的时候输出每一个动作的Q值，也就是输出一个向量<img src="http://www.zhihu.com/equation?tex=%5BQ%28s%2Ca_1%29%2CQ%28s%2Ca_2%29%2CQ%28s%2Ca_3%29%2C...%2CQ%28s%2Ca_n%29%5D"></img>，记住这里输出是一个值，只不过是包含了所有动作的Q值的向量而已。这样我们就只要输入状态s，而且还同时可以得到所有的动作Q值，也将更方便的进行Q-Learning中动作的选择与Q值更新。</div><div><br/></div><div><br/></div></div><div><br/></div><div><br/></div><div><br/></div><h2><span style="font-size: 24pt; color: rgb(50, 135, 18);">Q值神经网络化！</span></h2><div style="text-align: center;"><img src="DQN 从入门到放弃_files/Image [5].jpg" type="image/jpeg" data-filename="Image.jpg" width="1650"/></div><div><br/></div><div><ul><li>以DQN为例，<font style="font-size: 12pt;"><span style="font-size: 12pt; color: rgb(173, 0, 0); font-weight: bold;">输入是经过处理的4个连续的84x84图像</span></font></li><li>然后经过两个卷积层，两个全连接层</li><li>最后输出<font style="font-size: 12pt;"><span style="font-size: 12pt; color: rgb(173, 0, 0); font-weight: bold;">包含每一个动作Q值</span></font>的向量</li></ul></div><div><br/></div><div><br/></div><h2><span style="font-size: 24pt; color: rgb(50, 135, 18);">DQN算法</span></h2><div>        我们知道，神经网络的训练是一个最优化问题，最优化一个损失函数loss function，也就是标签和网络输出的偏差，目标是让损失函数最小化。</div><div>        为此，我们<font style="font-size: 12pt;"><span style="font-size: 12pt; color: rgb(173, 0, 0); font-weight: bold;">需要有样本，巨量的有标签数据</span></font>，然后通过反向传播使用梯度下降的方法来更新神经网络的参数。</div><div><font style="font-size: 24pt;"><span style="font-size: 24pt; color: rgb(50, 135, 18); font-weight: bold;">要训练Q网络，我们要能够为Q网络提供有标签的样本。</span></font></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 24pt; color: rgb(173, 0, 0); font-weight: bold;">如何为Q网络提供有标签的样本？</span></div><div>答案就是利用Q-Learning算法。</div><div style="margin-top: 1em; margin-bottom: 1em;">大家回想一下Q-Learning算法，Q值的更新依靠什么？依靠的是利用Reward和Q计算出来的目标Q值：</div><div><img src="http://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D%2B%5Clambda+%5Cmax+_aQ%28S_%7Bt%2B1%7D%2Ca%29"></img></div><div style="margin-top: 1em; margin-bottom: 1em;">因此，我们把目标Q值作为标签不就完了？我们的目标不就是让Q值趋近于目标Q值吗？</div><div style="margin-top: 1em; margin-bottom: 1em;">因此，Q网络训练的损失函数就是</div><div><img src="DQN 从入门到放弃_files/Image [6].jpg" type="image/jpeg" data-filename="Image.jpg" width="1412"/></div><div>上面公式是<img src="http://www.zhihu.com/equation?tex=s%5E%60%2Ca%5E%60"></img>即下一个状态和动作。这里用了David Silver的表示方式，看起来比较清晰。</div><div style="margin-top: 1em; margin-bottom: 1em;">既然确定了损失函数，也就是cost，确定了获取样本的方式。那么DQN的整个算法也就成型了！</div><div>接下来就是具体如何训练的问题了！</div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><h2><span style="font-size: 18pt; color: rgb(28, 51, 135);">DQN训练</span></h2><div>        算法主要涉及到Experience Replay，也就是经验池的技巧，就是<font style="font-size: 12pt;"><span style="font-size: 12pt; color: rgb(50, 135, 18); font-weight: bold;">如何存储样本及采样问题</span></font>。</div><div>        由于玩Atari采集的样本是一个时间序列，样本之间具有连续性，如果每次得到样本就更新Q值，受样本分布影响，效果会不好。因此，一个很直接的想法就是<font style="font-size: 12pt;"><span style="font-size: 12pt; color: rgb(173, 0, 0); font-weight: bold;">把样本先存起来，然后随机采样</span></font>如何？这就是Experience Replay的意思。</div><div>        那么上面的算法看起来那么长，其实就是反复试验，然后存储数据。接下来数据存到一定程度，就每次随机采用数据，进行梯度下降！</div><div>        </div><blockquote><div>在DQN中增强学习Q-Learning算法和深度学习的SGD训练是<font style="font-size: 12pt;"><span style="font-size: 12pt; color: rgb(173, 0, 0); font-weight: bold;">同步进行</span></font>的！</div></blockquote><div style="margin-top: 1em; margin-bottom: 1em;">通过Q-Learning获取<font style="font-size: 12pt;"><span style="font-size: 12pt; color: rgb(173, 0, 0); font-weight: bold;">无限量的训练样本</span></font>，然后对神经网络进行训练。<font style="font-size: 12pt;"><span style="font-size: 12pt; color: rgb(173, 0, 0); font-weight: bold; line-height: 1.45;">样本的获取关键是计算y，也就是标签。</span></font></div><div><br/></div><div><br/></div><div><br/></div><h2><span style="color: rgb(28, 51, 135); font-size: 18pt;">DQN算法用在连续控制上存在的问题</span></h2><div style="text-align: left;"><div>        DQN是一个面向离散控制的算法，也就是说输出的动作是离散的，不是连续的。那么，即使把每一个输出离散化，比如精度到0.01，那么一个动作有200个取值，那么6个关节也就是200的6次方个取值，这实在太多了。更何况如果进一步提升这个精度，那么取值的数量就成倍增加了。这就是连续控制比离散控制难得多的地方。将连续控制离散化也是完全不可取的做法</div></div><div>        么DQN为什么没办法直接用在连续控制上呢？原因很简单，DQN依靠计算每一个动作的Q值，然后选择最大的Q值对应的动作。那么这种方法在连续控制上完全不起作用。因为，根本就没办法穷举每一个动作，也就无法计算最大的Q值对应的动作。</div><div><br/></div></div><h2>Continuous Deep Q-Learning with NAF</h2><div>一种idea来实现连续控制</div><div><span style="font-weight: bold;">Step 1：在DQN的框架下，连续控制的输出需要满足什么条件？</span></div><div>因为DQN是通过计算Q值的最大值来选择动作。那么对于连续控制，我们已经无法选择动作，我们只能设计一种方法，使得<span style="font-weight: bold;">我们输入状态，然后能够输出动作，并且保证输出动作对应的Q值是最大值。</span></div><div><span style="font-weight: bold;">Step 2：又要输出动作，又要输出Q值？</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-weight: bold;">第一步的分析我们会发现一个两难的境地，就是我们输入状态，输出的时候，既要能输出动作，还要能输出Q值。那么这个时候，我们有两种选择，一种就是弄两个神经网络，一个是Policy网络，输入状态，输出动作，另一个是Q网络，输入状态，输出Q值。另外一种就是</span><span style="font-weight: bold;">弄一个神经网络，既输出动作，有能输出Q值</span><span style="font-weight: bold;">。先说第一种做法。这种做法其实就是Actor-Critic算法的做法。这种做法需要能够构建一个能够更新Policy网络的方法。而DQN并没有提供更新Policy网络的方法。这使得我们要基于DQN做文章，只有一个办法，就是只弄一个神经网络，既能输出动作也能输出Q值。But，how？</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-weight: bold;">Step 3：如何构建神经网络，又能输出动作，也能输出Q值，而且动作对应的Q值最大？</span></div><div><span style="font-weight: bold;">这个问题确实是很困难的一个问题，很难直接就想出一个好的做法。虽然在Paper中作者其实只用了一段话来说明他们的方法，但是确实是很酷的idea。先提一下这些作者</span><a href="http://link.zhihu.com/?target=http%3A//arxiv.org/find/cs/1/au%3A%2BGu_S/0/1/0/all/0/1" style="font-weight: bold;">Shixiang Gu</a><span style="font-weight: bold;">,</span> <a href="http://link.zhihu.com/?target=http%3A//arxiv.org/find/cs/1/au%3A%2BLillicrap_T/0/1/0/all/0/1" style="font-weight: bold;">Timothy Lillicrap</a><span style="font-weight: bold;">,</span> <a href="http://link.zhihu.com/?target=http%3A//arxiv.org/find/cs/1/au%3A%2BSutskever_I/0/1/0/all/0/1" style="font-weight: bold;">Ilya Sutskever</a><span style="font-weight: bold;">,</span> <a href="http://link.zhihu.com/?target=http%3A//arxiv.org/find/cs/1/au%3A%2BLevine_S/0/1/0/all/0/1" style="font-weight: bold;">Sergey Levine</a> <span style="font-weight: bold;">后面两个都很牛。然后我们还是直接分析他们提出的方法吧！</span></div><div style="text-align: center;"><img src="DQN 从入门到放弃_files/Image [7].jpg" type="image/jpeg" data-filename="Image.jpg" style="font-weight: bold;" width="720"/></div><div style="text-align: left;">1）State，维度是输入维度state_dim</div><div style="margin-top: 1em; margin-bottom: 1em; text-align: left;">2) 经过两个200的RELU全连接层</div><div style="margin-top: 1em; margin-bottom: 1em; text-align: left;">3）输出V，维度为1</div><div style="margin-top: 1em; margin-bottom: 1em; text-align: left;">4）输出mu（动作），维度为动作的维度action_dim</div><div style="margin-top: 1em; margin-bottom: 1em; text-align: left;">5）输出L0,维度为（action_dim)x(action_dim+1)/2，也就是构造下三角矩阵L所需要的维度</div><div style="margin-top: 1em; margin-bottom: 1em; text-align: left;">6）构造L。将L0转化为L.也就是将一个列向量转换为下三角矩阵，就是从新排列，然后把对角线的数exp对数化。</div><div style="margin-top: 1em; margin-bottom: 1em; text-align: left;">7）根据L构造P。</div><div style="margin-top: 1em; margin-bottom: 1em; text-align: left;">8）根据mu，P，action构造A</div><div style="margin-top: 1em; margin-bottom: 1em; text-align: left;">9）根据A和V构造Q，也就是Q=A+V</div><div style="text-align: left;"><div>综上，最终输出Q，并且可以根据DQN的方法进行梯度下降。</div></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div></div></span>
</div></body></html> 